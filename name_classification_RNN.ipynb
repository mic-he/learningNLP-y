{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to classify surnames with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the tutorial available at: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a classifier for surnames from scratch (pre-processing included), using character-level RNN, which predicts which among 18 languages the surname is most likely from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "The data is available at: https://download.pytorch.org/tutorial/data.zip\n",
    "\n",
    "The folder `data/names` contains 18 Unicode text files, one for each of 18 languages, with names such as `[Language].txt`. With the following code we read the files, conver the data to ASCII and create a dictionary with shape `{language: [name1, name2, ...]}` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a few packages\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Greek.txt', 'data/names/German.txt', 'data/names/Portuguese.txt', 'data/names/Irish.txt', 'data/names/Scottish.txt', 'data/names/Czech.txt', 'data/names/English.txt', 'data/names/Vietnamese.txt', 'data/names/Polish.txt', 'data/names/Korean.txt', 'data/names/French.txt', 'data/names/Spanish.txt', 'data/names/Arabic.txt', 'data/names/Chinese.txt', 'data/names/Dutch.txt', 'data/names/Japanese.txt', 'data/names/Italian.txt', 'data/names/Russian.txt']\n"
     ]
    }
   ],
   "source": [
    "# get paths of the language files \n",
    "def findFiles(path): return glob.glob(path)\n",
    "print(findFiles('data/names/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\" # define the alphabet\n",
    "n_letters = len(all_letters) # size of alphabet\n",
    "\n",
    "# from Unicode string to plain ASCII, following https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# for example:\n",
    "print(unicodeToAscii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to read a file, convert and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dictionary, a list of names for each language\n",
    "category_dict = {} # init empty dict\n",
    "all_categories = [] # init empty list\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0] # get language name from file path\n",
    "    all_categories.append(category)\n",
    "    names = readLines(filename)\n",
    "    category_dict[category] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "dict_keys(['Russian', 'Dutch', 'Scottish', 'Arabic', 'Spanish', 'English', 'Chinese', 'German', 'Japanese', 'French', 'Portuguese', 'Greek', 'Polish', 'Korean', 'Vietnamese', 'Italian', 'Czech', 'Irish'])\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "# inspect the data a bit\n",
    "n_categories = len(all_categories)\n",
    "print(n_categories)\n",
    "print(category_dict.keys())\n",
    "print(category_dict['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From names to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one-hot encoding for the letters: each letter is encoded in a vector of 0s and 1s, whose length is equal to the cardinality of the alphabet; for example: letter `a = [1,0,0,0,...]`, letter `b = [0,1,0,0,...]` and so on. \n",
    "\n",
    "The encoding of a word is then obtained as a matrix whose rows are the encodings of the letters of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function to find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# example\n",
    "letterToIndex(\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first import torch to define tensors\n",
    "import torch\n",
    "\n",
    "# define a function to turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters) # first, all positions to 0; 1 as first argument is dimension of tensor\n",
    "    tensor[0][letterToIndex(letter)] = 1 # then set the index position to 1\n",
    "    return tensor\n",
    "\n",
    "# example:\n",
    "letterToTensor(\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function to convert a word into a matrix of one-hot encodings\n",
    "def wordToTensor(word):\n",
    "    tensor = torch.zeros(len(word), 1, n_letters)\n",
    "    for li, letter in enumerate(word):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "#example:\n",
    "wordToTensor(\"hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN is quite simple: just 2 linear layers which operate on an input and hidden state, with a LogSoftmax layer after the output (see https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # we customize the nn.Module\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) # layer from input to hidden state\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size) # layer from input to output\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1) # the source of recurrency: input and hidden are combined here\n",
    "        hidden = self.i2h(combined) # the result is fed to linear transformation going to hidden layer itself\n",
    "        output = self.i2o(combined) # and to output layer\n",
    "        output = self.softmax(output) # which is then softmax-transformed to probability over categories\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self): # init hidden layer with zeros\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network setting a few parameteres\n",
    "n_hidden = 128 # size of hidden layer\n",
    "\n",
    "rnn = RNN(n_letters, n_hidden, n_categories) # n_categories is defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a forward pass of the network, i.e. with a letter-tensor and \"empty\" hidden state as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8504, -2.9096, -2.9647, -2.8908, -2.9219, -2.9769, -2.8536, -2.9881,\n",
      "         -2.9007, -2.9706, -2.9378, -2.9961, -2.7893, -2.8977, -2.9455, -2.7810,\n",
      "         -2.7332, -2.7745]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = letterToTensor('A')\n",
    "hidden =torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way to do this is to compute the word-tensor once and using slices for each letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8504, -2.9096, -2.9647, -2.8908, -2.9219, -2.9769, -2.8536, -2.9881,\n",
      "         -2.9007, -2.9706, -2.9378, -2.9961, -2.7893, -2.8977, -2.9455, -2.7810,\n",
      "         -2.7332, -2.7745]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = wordToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a couple of helper functions. The first of them makes it easier to interpret the output of the nn: it takes the probability distribution yielded by softmax and returns the most likely language according to the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Italian', 16)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1) # .topk is a method of tensors\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function which yields a random training example (a name with its language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l): # extract a random element from a list\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories) # extract a random language\n",
    "    word = randomChoice(category_dict[category]) # extract a random name from the language\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long) # language index as tensor\n",
    "    word_tensor = wordToTensor(word) # the tensor associated with the word\n",
    "    return category, word, category_tensor, word_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Scottish',\n",
       " 'Kennedy',\n",
       " tensor([4]),\n",
       " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language = Spanish / name = Capello\n",
      "language = Russian / name = Mokrov\n",
      "language = English / name = Leggett\n",
      "language = German / name = Hummel\n",
      "language = Spanish / name = Puerta\n",
      "language = Korean / name = Nam\n",
      "language = German / name = Winther\n",
      "language = English / name = Kinniburgh\n",
      "language = Czech / name = Pfeifer\n",
      "language = Korean / name = Jeong\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    category, word, category_tensor, word_tensor = randomTrainingExample()\n",
    "    print('language =', category, '/ name =', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move to training the nn. We use negative likelihood as loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005 # how to optimize this hyper-par?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function which takes a training example <name,language>, compute nn's prediction given the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(category_tensor, word_tensor):\n",
    "    hidden = rnn.initHidden() # initialize hidden ;ayer\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(word_tensor.size()[0]): # feed forward for each character of the name\n",
    "        output, hidden = rnn(word_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor) # compute loss\n",
    "    loss.backward() # and its gradients\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate: update weights\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean\n",
      "tensor([[-2.8303, -2.9692, -3.0209, -2.8446, -2.8343, -3.0171, -2.7709, -2.8889,\n",
      "         -2.8419, -2.9014, -2.9655, -3.0324, -2.7673, -2.8733, -2.9639, -2.8689,\n",
      "         -2.8727, -2.8211]], grad_fn=<LogSoftmaxBackward>)\n",
      "('Arabic', 12)\n",
      "2.9014081954956055\n"
     ]
    }
   ],
   "source": [
    "category, word, category_tensor, word_tensor = randomTrainingExample()\n",
    "output, loss = train(category_tensor, word_tensor)\n",
    "print(category)\n",
    "print(output)\n",
    "print(categoryFromOutput(output))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we loop the `train` function, iteratively updating the weights and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 17s) 2.5013 Donndubhan / Scottish ✗ (Irish)\n",
      "10000 10% (0m 34s) 2.7209 Sarkis / Greek ✗ (Arabic)\n",
      "15000 15% (0m 52s) 2.7088 Shima / Japanese ✗ (Czech)\n",
      "20000 20% (1m 10s) 0.7554 Vyuchnov / Russian ✓\n",
      "25000 25% (1m 30s) 0.2493 Sakata / Japanese ✓\n",
      "30000 30% (1m 50s) 1.1169 Fiscella / Italian ✓\n",
      "35000 35% (2m 7s) 0.5386 Idoni / Italian ✓\n",
      "40000 40% (2m 26s) 1.4933 Herrera / Dutch ✗ (Spanish)\n",
      "45000 45% (2m 44s) 0.0638 Nakasawa / Japanese ✓\n",
      "50000 50% (3m 2s) 1.7330 Taverna / Spanish ✗ (Italian)\n",
      "55000 55% (3m 22s) 0.6066 Halyavin / Russian ✓\n",
      "60000 60% (3m 41s) 2.9477 Gouveia / Russian ✗ (Portuguese)\n",
      "65000 65% (4m 0s) 0.1135 Paitakes / Greek ✓\n",
      "70000 70% (4m 21s) 0.0498 Okasawa / Japanese ✓\n",
      "75000 75% (4m 39s) 0.2252 Hamitsev / Russian ✓\n",
      "80000 80% (4m 58s) 1.6833 Snelker / Dutch ✗ (Czech)\n",
      "85000 85% (5m 17s) 0.2644 Riagain / Irish ✓\n",
      "90000 90% (5m 35s) 0.0837 Janowski / Polish ✓\n",
      "95000 95% (5m 52s) 2.2768 Grant / French ✗ (Scottish)\n",
      "100000 100% (6m 11s) 2.0131 Shamoon / Russian ✗ (Arabic)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000 # we don't want to print or plot every value\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the history of loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f43590fd518>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4lNeZ9/Hvrd5RRQhVEFVUgejVFZu4l8QNO27Ysd/Ezsapm03ZbDbJJnGcrGPjGscxixPbxHbcEmODMR3Ri+hCICFQQwUJoXa/f8wgI6EGjBhp5v5cF5elmTMz9/iB3zxzznnOEVXFGGOMZ/FxdwHGGGNcz8LdGGM8kIW7McZ4IAt3Y4zxQBbuxhjjgSzcjTHGA1m4G2OMB7JwN8YYD2ThbowxHsjPXS8cGxuraWlp7np5Y4zplTZs2FCiqnGdtXNbuKelpZGdne2ulzfGmF5JRPK60s66ZYwxxgNZuBtjjAeycDfGGA9k4W6MMR7Iwt0YYzyQhbsxxnggC3djjPFAvS7cD5fV8NN/7KC+scndpRhjTI/V68J999Eq/rTyIIvWHXJ3KcYY02P1unC/bHhfJg+M5qkle6msrXd3OcYY0yP1unAXEf59bgZl1XUsWLbf3eUYY0yP1OvCHWBUUh9uzEzkpRW5FJSfdHc5xhjT4/TKcAd4Ys5QFPjtP3e7uxRjjOlxem24J0YGc//0ASzeVMDGQ8fdXY4xxvQonYa7iCSLyFIR2SkiO0TksTba9BGRf4jIFmebe7un3JYemZ1O/z5BPPHGFmrrGy/GSxpjTK/QlTP3BuBbqpoBTAYeFZGMVm0eBXaq6hhgNvBbEQlwaaVtCA/y539uGcOB4mp++y/rnjHGmNM6DXdVLVTVjc6fq4AcILF1MyBcRAQIA8pwfCh0u+mDY7lrcgovrshl/cGyi/GSxhjT451Tn7uIpAGZwNpWdz0NDAeOANuAx1T1ol1C+v2rh5MUFcwTb2yhpu6ifKYYY0yP1uVwF5Ew4C3gcVWtbHX3HGAz0B8YCzwtIhFtPMd8EckWkezi4uILKLul0EA//ufmMeSV1vDKqoMue15jjOmtuhTuIuKPI9gXquriNprcCyxWh31ALjCsdSNVfV5Vs1Q1Ky6u0/1dz8mU9BhmDI7l5RUHbXDVGOP1ujJbRoCXgBxVfbKdZoeAy5zt44GhwAFXFdlVj8weRMmJU7yxIf9iv7QxxvQoXTlznwbMAy4Vkc3OP3NF5GERedjZ5mfAVBHZBnwCfFdVS7qp5nZNHhhNZkokzy/fT4OtGmmM8WJ+nTVQ1RWAdNLmCHClq4o6XyLCI7MH8eCr2by3tZAbMltP6jHGGO/Qa69Qbc9lw/oyJD6MZ5ftp6lJ3V2OMca4hceFu4+P8LXZ6ew+VsWyPUXuLscYY9zC48Id4JrR/ekbHsjCNbahhzHGO3lkuPv7+nBrVhJLdxdRWGFLAhtjvI9HhjvAV7JSaFL423qbFmmM8T4eG+4pMSFMHxTL37IP02gDq8YYL+Ox4Q5w+8QUCspPsnyv65Y6MMaY3sCjw/2KjHhiQgN4fZ0NrBpjvItHh3uAnw83j0/ik5wiiqpq3V2OMcZcNB4d7gBfmZBMQ5Py7uYj7i7FGGMuGo8P9/S4MFJjQmwjD2OMV/H4cAcYnxrFhrzjqNqsGWOMd/CacC85UUdeaY27SzHGmIvCK8I9KzUagA15x91ciTHGXBxeEe6D+4YRHuRHtoW7McZLdGUnpmQRWSoiO0Vkh4g81k672c6NPHaIyGeuL/X8+fgI41Ki2GjhbozxEl05c28AvqWqGcBk4FERyTizgYhEAs8A16nqCOBWl1d6gcanRrGnqIqKk/XuLsUYY7pdp+GuqoWqutH5cxWQA7Te4ugOHBtkH3K263ELqWelRqEKmw7Z2bsxxvOdU5+7iKQBmcDaVncNAaJEZJmIbBCRu11TnuuMSY7E10dsUNUY4xU63UP1NBEJA94CHlfVyjaeZzxwGRAMrBaRNaq6p9VzzAfmA6SkpFxI3ecsNNCP4QnhZB+0cDfGeL4unbmLiD+OYF+oqovbaJIP/FNVq1W1BFgOjGndSFWfV9UsVc2Ki4u7kLrPy/iUKDYfLqehsemiv7YxxlxMXZktI8BLQI6qPtlOs3eA6SLiJyIhwCQcffM9yvi0aE7WN5JTWOXuUowxplt1pVtmGjAP2CYim523/QBIAVDVBaqaIyIfAVuBJuBFVd3eHQVfiKzUKACy88oYldTHzdUYY0z36TTcVXUFIF1o92vg164oqrv0jwymX0QQmw6Vc+80d1djjDHdxyuuUD1TZkokmw+Xu7sMY4zpVl4X7mOTIzlUVkPpiVPuLsUYY7qNV4Y7YGfvxhiP5nXhPiqpD74+YuFujPFoXhfuIQF+DI0PZ9MhC3djjOfyunAHGJsSyZbD5TQ12c5MxhjP5JXhnpkcSdWpBg6UnHB3KcYY0y28M9xTHIOqG61rxhjjobwy3AfGOnZmskFVY4yn8spw9/ERxiRFstnO3I0xHsorwx0cXTO7j1VRU9fg7lKMMcblvDbcxyZH0tikbMuvcHcpxhjjcl4d7mBXqhpjPJPXhntMWCBpMSGsyy1zdynGGONyXhvuANMHx7L6QCl1DbYzkzHGs3h1uM8cHEdNXSPZeXb2bozxLF3ZZi9ZRJaKyE4R2SEij3XQdoKINIjILa4ts3tMHRSLn4+wfE+Ju0sxxhiX6sqZewPwLVXNACYDj4pIRutGIuIL/Ar4l2tL7D5hgX6MT41i+Z5id5dijDEu1Wm4q2qhqm50/lyFY+PrxDaafh14CyhyaYXdbOaQOHYWVlJUVevuUowxxmXOqc9dRNKATGBtq9sTgRuBZzt5/HwRyRaR7OLinnG2PGtIHACfW9eMMcaDdDncRSQMx5n546pa2erup4DvqmqH005U9XlVzVLVrLi4uHOvthtkJEQQGxbA8r0948PGGGNcwa8rjUTEH0ewL1TVxW00yQJeFxGAWGCuiDSo6tsuq7Sb+PgIMwbH8dmeYpqaFB8fcXdJxhhzwboyW0aAl4AcVX2yrTaqOkBV01Q1DXgTeKQ3BPtpM4fEUlZdx/YjthSBMcYzdOXMfRowD9gmIpudt/0ASAFQ1QXdVNtFM2Owo4to+Z5iRidFurkaY4y5cJ2Gu6quALrcV6GqX72QgtwhNiyQMcmRvL35CI/MHmRdM8aYXs+rr1A9071T09hXdIJPdvWqmZzGGNMmC3ena0YnkBQVzLPL9qFqG2cbY3o3C3cnP18f5s8cyMZD5aw/eNzd5RhjzAWxcD/DreOTiQ4N4Nll+9xdijHGXBAL9zMEB/hy79Q0lu4uJqew9XVaxhjTe1i4t3L3lDRCA3x5YfkBd5dijDHnzcK9lT4h/swdlcCSnGM0NtnAqjGmd7Jwb8P0wbFU1jaww65YNcb0UhbubZiaHgvAin22UqQxpneycG9DXHggw/qFs9LC3RjTS1m4t2PaoFjWHzxObX2ju0sxxphzZuHejumDYqlraCLbLmgyxvRCFu7tmDggGj8fsX53Y0yvZOHejtBAP8alRFm/uzGmV7Jw78C0QbFsP1LB8eo6d5dijDHnpCs7MSWLyFIR2SkiO0TksTba3CkiW0Vkm4isEpEx3VPuxTV9cAyqsPpAqbtLMcaYc9KVM/cG4FuqmgFMBh4VkYxWbXKBWao6CvgZ8Lxry3SP0UmRhAX6Wb+7MabX6TTcVbVQVTc6f64CcoDEVm1WqerpaSVrgCRXF+oO/r4+TBsUw9ubCvhwW6G7yzHGmC47pz53EUkDMoG1HTS7H/iwncfPF5FsEckuLi4+l5d2m59eN5Ih8eF8beFGfvFBDg2NTe4uyRhjOtXlcBeRMOAt4HFVbXM9XBG5BEe4f7et+1X1eVXNUtWsuLi486n3ouvXJ4i/PjSZOyel8NzyAzz82kbbqckY0+N1KdxFxB9HsC9U1cXttBkNvAhcr6oeNQIZ6OfLz28cxRNXDmFJzjHW5Za5uyRjjOlQV2bLCPASkKOqT7bTJgVYDMxT1T2uLbHneGDGQKJDA3je1no3xvRwXTlznwbMAy4Vkc3OP3NF5GERedjZ5kdADPCM8/7s7irYnYL8fbl7Siqf7CpiX1GVu8sxxph2+XXWQFVXANJJmweAB1xVVE82b3Iqzy7bzwvLc/nVLaPdXY4xxrTJrlA9RzFhgdyalcTfNxVQVFnr7nKMMaZNFu7n4YHpA6lvauKVVQfdXYoxxrTJwv08pMWGctWIfvxldZ71vRtjeiQL9/P0g7nDCfT34e6X1nG0wrpnjDE9i4X7eUqODuGVeydSWdvAPS+vo6Km3t0lGWNMMwv3CzAysQ/PzxvPgZIT3PvKOg6V1ri7JGOMASzcL9jUQbH8/rZMcgqruPzJz/jvD3KoOGln8cYY97Jwd4G5oxJY9u3ZXD+2Py98foArf/cZlbUW8MYY97Fwd5H4iCB+fesY/njHOI5VnmLzoXJ3l2SM8WIW7i42NT0GgJ2FbS6caYwxF4WFu4tFhgTQv08QORbuxhg3snDvBhn9I9h5xMLdGOM+Fu7dICMhgv3FJ6itb3R3KcYYL2Xh3g0y+kfQpLDnmC1NYIxxDwv3bjA8IQLAumaMMW7TlZ2YkkVkqYjsFJEdIvJYG21ERP4gIvtEZKuIjOuecnuH5KgQwgL9bMaMMcZtOt2sA2gAvqWqG0UkHNggIh+r6s4z2lwNDHb+mQQ86/yvV/LxEYYnhNuZuzHGbTo9c1fVQlXd6Py5CsgBEls1ux54VR3WAJEikuDyanuRjIQIdh2toqlJ3V2KMcYLnVOfu4ikAZnA2lZ3JQKHz/g9n7M/ALzK8IQITpxq4PBxW0zMGHPxdTncRSQMeAt4XFXPq79BROaLSLaIZBcXF5/PU/QaGf3bH1RtalJyCivtrN4Y0226FO4i4o8j2Beq6uI2mhQAyWf8nuS8rQVVfV5Vs1Q1Ky4u7nzq7TWGxIfj6yMtrlQtOXGKZ5ftZ/ZvlnH17z/ntbV5bqzQGOPJujJbRoCXgBxVfbKdZu8CdztnzUwGKlS10IV19jpB/r6kx4U2z5h5I/swU3/5Kb/6aBcJfYJIjwvllZUH7ezdGNMtujJbZhowD9gmIpudt/0ASAFQ1QXAB8BcYB9QA9zr+lJ7n+EJEazLLeMXH+Tw3PIDTBsUw0+uHcHg+HD+vimfb/51Cyv3lzBjsGd/izHGXHydhruqrgCkkzYKPOqqojxFRkIE72w+wnPLDzBvcio/ujYDf1/Hl6W5oxL4+fs5/HnVQQt3Y4zL2RWq3WjaoFjCg/z4z+tH8LMbRjYHO0Cgny+3T0zhk11FHC6zGTXGGNeycO9GIxP7sPXHV3L3lLQ2779jUgo+Iry2xgZWjTGuZeHezRzj0W1L6BPMnBHxvL7+MEfKT7LzSCXL9xRTXHXqIlZojPFEXRlQNd3onilpfLDtKFN/+Wnzbf6+wtUjE7hrcioT0qI6/IAwxpi2WLi72cQB0fz3jaOoa2gkPiKIPsH+fJxzjDc35PPuliN8e85QHr1kkLvLNMb0MuKY6HLxZWVlaXZ2tlteuzeoqWvg3j+t52hlLcuemG1n78YYAERkg6pmddbO+tx7qJAAP64d05+80hr2Fp1wdznGmF7Gwr0HuyIjHoCPdx5zcyXGmN7Gwr0Hi48IYkxSH/5l4W6MOUcW7j3cFRnxbDlczrHKWneXYozpRSzce7grMvoBsCTHzt6NMV1n4d7DDYkPIyU6xPrdjTHnxMK9hxMRrsiIZ9W+Uk6canB3OcaYXsIuYuoFrsiI56UVufx51UEC/XzYml/BtEExfGVCirtLM8b0UBbuvUBWahRRIf78+p+7AQjw9WFdbhlfzkq2i5uMMW3qNNxF5GXgGqBIVUe2cX8f4DUcm3f4Ab9R1T+5ulBv5ufrw8tfncCxylNkpkSydFcR31u8jX1FJxgcH+7u8owxPVBX+txfAa7q4P5HgZ2qOgaYDfxWRAIuvDRzpsyUKK4a2Y/4iCBmDHFs7vHZHs/eZNwYc/46DXdVXQ6UddQECHfutRrmbGsjf90oMTKY9LhQPt9b4u5SjDE9lCtmyzwNDAeOANuAx1S1yQXPazowY3Aca3NLqa1vdHcpxpgeyBXhPgfYDPQHxgJPi0hEWw1FZL6IZItIdnGxdSlciJlDYqmtbyL74HF3l2KM6YFcEe73AovVYR+QCwxrq6GqPq+qWaqaFRdnm0JfiMkDY/D3FT7f2/mHZG19I3uOVV2EqowxPYUrwv0QcBmAiMQDQ4EDLnhe04GQAD+yUqM7HVRVVb6xaBNznlrOJ7aEgTFeo9NwF5FFwGpgqIjki8j9IvKwiDzsbPIzYKqIbAM+Ab6rqjbSdxHMHBLHrqNVFHWwqNhbGwv4185jRAT58/jrmzlQbGvDG+MNujJb5nZVTVBVf1VNUtWXVHWBqi5w3n9EVa9U1VGqOlJVX+v+sg3AjMGxAHy+t4S80mp+v2Qvv/nnbspr6gDIP17DT97dwcQB0bz39en4+/nw0F822DIGxngBu0K1F8tIiCAmNICf/GMHVbUNiIAAC9fm8cScofxjyxFUld/eOobk6BCeviOTeS+t44m/beHZu8bZ1a3GeDBbOKwX8/ER7piUQlJUCN+5aigrv3sp739jBoPjw/n3v29nzYEyfnztCJKjQwCYmh7LNy8fzEc7jpJT2PYA64lTDbz4+QFufnYV+4psENaY3so2yPZAqsp7WwvJP36Sh2cNbHGGXlhxkim/+JQffmk4D8wY2Hx7Q2MTT368h7+syaOq1tFt8/CsdL53dZsTn4wxbmIbZHsxEeHaMf352uz0s7peEvoEMzA2lJX7Wo55v7+tkGeW7WdaeizvPDqNyQOjWba76GKWbYxxIQt3LzR1UAzrcsuob/ziQuKPth+lb3ggz9w5jjHJkVwytC+7jlZRWHHSjZUaY86XhbsXmpYeS3VdI1sOlwOOi5yW7S7miox4fHwcZ/qXDOsLwLLddiWxMb2RhbsXmpIegwis3FcKwIq9JZysb2TOiH7NbQb3DSMxMpilu6xrxpjeyMLdC0WGBDCifwQr9zv63f+54yjhQX5MHhjT3EZEmDU0jpX7SqhrsHXgjOltLNy91LT0WDYdOk5VbT1Lco5x6bC+BPi1/OtwydC+VNc1kn3QseJzQflJLvvtMv6x5Yg7SjbGnAMLdy81dVAs9Y3Ks8v2c7ymvkWXTHOb9BgCfH1YuruIk3WNPPSXbPYXV/Ph9kI3VGyMORd2haqXmpAWhb+v8OLnuQT4+TBryNmrdIYG+jFpYDSf7iriWOUpdhypJD0ulHW5x1FVu8LVmB7Mzty9VEiAH5kpUdQ1NjFzcCyhgW1/zs8e2pf9xdW8u+UIT1w5lPumD6DkxCnySmsucsXGmHNh4e7FpqU7Fh67so0umdMudU6JvGZ0Ao/MTmdiWjQA6w52tPOiMcbdrFvGi900LpHdxyq5amT74T4gNpT3vj6dIfHhiAiD+oYRFeLP+twyvpyVfBGrNcZ1Fm/Mp6q2gXumprm7lG5j4e7FkqNDeObO8Z22G5nYp/lnESErLZr1duZuerFF6w6xt+gE8yanNl+452m6slnHyyJSJCLbO2gzW0Q2i8gOEfnMtSWanmZiWjQHS2soqmp/kxBjerKy6jrKa+rZW+S5m9d0pc/9FeCq9u4UkUjgGeA6VR0B3Oqa0kxPNWGAo999fa5tzm16p7Jqx4Y263JL3VxJ9+m0W0ZVl4tIWgdN7sCxQfYhZ3u7Xt3DjegfQbC/L+sPlvGl0Qkt7tteUMFP/7GDvUUn8Pf1IcDXhy+NTuAHc4e7qVpjWmpsUspP1gOwNreMeVPS3FtQN3HFbJkhQJSILBORDSJytwue0/Rg/r4+jEuNZF3uF/3ulbX1/OTdHVz39ApyS2q4ZnQClw/vS9+IQF5akcuR8vZXl9xeUMFzn+23ZQ7MRVFeU4cq+Aisyy3DXXtadDdXDKj6AeOBy4BgYLWIrFHVPa0bish8YD5ASkqKC17auEtWajR/+HQvlbX1rNlfyg/f3k7xiVPMm5zKt64cSp9gfwAOl9Uw69dLWbg2j2/POXvjjy2Hy7nrxbVUnWrg011FPHvXeKJDAy722zFe5HSXzIS0aNbmlpFXWkNabKibq3I9V5y55wP/VNVqVS0BlgNj2mqoqs+rapaqZsXFnX1FpOk9Jg6IRhXmvbiW+X/ZQHRoAG8/Mo3/vH5kc7CDY0bOZcPjWbTuMLX1jS2eY3tBBfNeWktkqD8/uiaDTYfLueGPK9lzzLb3M92n1BnuVzunAJ/5DdSTuCLc3wGmi4ifiIQAk4AcFzyv6cEyUyLx9xVyCqv41hVDePf/TWdMcmSbbb86NY2y6jre2/rFmjTbCyq488W1hAf5s+jBydw3fQB/nT+Zk/WN3LpgNSdONXSpjqYmZemuorM+OIxpz/HTZ+4DookODWCtt4a7iCwCVgNDRSRfRO4XkYdF5GEAVc0BPgK2AuuAF1W13WmTxjOEBPjx6n2T+OCxGXz9ssFnrSh5pqnpMQzqG8afVx1EVVmXW8btL6whNMCX1+dPJinKsYF3ZkoUv711DBUn69mQ17WZOE99spd7X1nP05/uc8n7Mp7v9Jl7TGggE9OiWXfQM2fMdBruqnq7qiaoqr+qJqnqS6q6QFUXnNHm16qaoaojVfWp7i3Z9BRTnKHdGRHhnimpbCuo4Pef7GXeS2uJCw/kja9NJTk6pEXbrLQo/HykS1PU3t9ayB8+2Uuwvy+vrc3jZJ2dvZvOne5zjwr1Z+KAaA6XnexwwL+3srVlzEVx07gkwgP9eGrJXobEh/PGQ1NIjAw+q11IgB8jEvt0Ood+e0EF33pjM+NTo3jxnizKa+p5c2N+d5Xfps2Hy7n9+TXU1HWtC8n0DGXVdYQH+hHo58vE09dseOAV1xbu5qIIDfTj364cwrVj+vN/D04iJiyw3baTBkSz+XB5u/3oJSdO8eCr2USHBLDgrvFMTY9hTFIfXl6RS1PTuU1re2H5Af686uA5Pea0v2/MZ/WBUjbmlZ/X4417lFXXER3mmJE1PCGC8EA/j+x3t3A3F8290wbwv7dnEh7k32G7CWnR1DU2NW/gfaamJuWbf91MWXUdz9+dRVx4ICLCAzMGkltSzafnsOfrmgOl/PyDHH723k4Ol537EsarDzi6jjYesit1e5Oy6jqiQhzh7usjjE+Lat5tzJNYuJseZ0JaFND2V+VnP9vP53tL+PG1I1osaHb1yH4kRgbzwucHuvQatfWN/GDxNhIjg/HxEf7wyd4W9xdV1bLjSEW7jy85cYo9xxzrkmyycO9VyqrriDnjWoqMhAgOFFdT3+hZF9FZuJseJzIkgGH9ws/6qrz+YBlPfryHa8f05/aJLZcb9vP14d5paazNLWNbfvuhfNofl+7jQEk1v7x5FHdOSmHxpgJyS6oBKD1xipufXcV1T69sd7/YtQcctQ3rF86mw+Uee5WjJyqrrmtxoVx6XBgNTXpe3956Mgt30yNNSItmY95xGpxnU2XVdXz9/zaRHBXMf984ss0t/r48IZmQAF9eW5PX4XPvPlrFs8v2c1NmIjMGx/G12ekE+Prw+yV7qK1v5MFXsymuOsWI/hE89vom3t5UcNZzrD5QQmiAL3dNTqW8pr75g8H0bKp6VrgPjHNcnbq/2LOOoYW76ZEmDoimuq6RnYWVqCrffWsrZdV1PH3HuHb77COC/LlmdALvbT1CdTsXQW0vqOCRhRuICPbnh9dkANA3PIi7p6byzpYj3PfKejYdLuepr4zl9fmTmTQghm/+bTNvbmg5E2f1/lImDIhunm2x6ZANqvYG1XWN1DU2tQp3x3Te/cWetfyvhbvpkU6H5rrcMhatO8zHO4/xnauGtuhnb8uXs5Kprmvk/W2FLW4/WdfILz7I4fo/rqSytoGnb89s8Q/8oZnphPj7smp/Kd+/ehhXjUwgJMCPl786ganpMXzvra0cdJ6dF1XVsr+4mskDYxgUF0Z4oJ8NqvYSZSccc9zPPPZ9gv2JCw9kv4et7W7hbnqk+IggUmNCeHtzAf/53g5mDI7lvmkDOn3c+NQoBsaF8rf1h5tvO1nXyI3PrOS55Qe4dXwSS/5tFlMHxbZ4XHRoAL+8eTTfv3oYD84Y2Hx7cIAvv/vyWPx9ffj1P3cDsMbZ3z5lYAw+PsLYlEg7c+8lSqtPAZy1OF16XCgHPKxrzcLd9FgT06LZXlBJsL8vv711TJe2QxMRvpKVTHbe8eav2b/4MIddR6t4bt54fnnz6BYLm53p2jH9eWhW+ln9+X0jgnhw5kDe31bIpkPHWb2/lPBAP0b0jwAgMzmSXUcr2+0KMj3H8Zqzz9zB0TWzr+iERw2MW7ibHmv6YMfZ9a9uHk3fiKAuP+7GcYn4+gh/yz7M0t1FvLo6j/unD2DOiPY3Au/M/JkDiQ0L4Bcf7mLNgVImDojGz9fxzyczNYomha1dmKVzpk93HaP0xKnzrsmcu9ITX6wrc6b0uDAqTtY3L03gCSzcTY917ej+LP/2JVx5jqHcNzyIS4f15a0N+Xznza0MjQ/n23OGXlAtYYF+PHb5ENbllpFbUs2U9Jjm+zKdq2FuOtz1fvfP9xZz3yvZPLe8a/PyL5aq2nqPOntt7fSZe1Roy29v6ecwY6ahsYnV+3v+YmMW7qbH8vERUmJCOm/Yhq9kJVNyoo6Kmnp+95WxBPn7XnA9t01IZqBzU4fJA78I98iQAAbGhXZ5GYJTDY386J0dAF1e/bK1rfnlfLan+Lwe255DpTWM/68l/HPHUZc+b09SWl1HgK8PYYEt9ylKP4cZMy+tyOX2F9aw62hlt9ToKhbuxiPNHhrHzCFx/Of1I8hw9o1fKH9fH35+4yhuzExkeELL58xMjmLz4eNdOut97rMD5JZUMyY5km35FZxqOLfVLD/YVsgtz67m/lfWs/uo6zY2Wbwpn7qGJpbvLXHZc/Y0ZSccc9xbj6skRgYT6OfDgU7CvaGxqXktopxCC3djLjo/Xx9evW8it0107XaOU9Jj+N1XxuLbanB3XGokJSfqWLmv46/reaXVPL055B9lAAATrElEQVR0H9eMTuBrswZS19jEjiNdD4lXVx/k0f/byMjECMKD/Pj+4q3nvFhaW1SVdzY7rsbdeJ7fJnqD4zV1bW7j6OMjDIgN7bRb5l87j3GkohaA3Ud79tRJC3djXOBLoxIY1DeMB15dz4p2znzrG5v4j3d2EODrw39ck8G4FMcaOl0N0xeWH+BH7+zgsmF9WfjAZH74pQw2Hipn4bpDF1z/lvwKckuqGRAbyu5jVVTW1l/wc/ZEpdVthztAet+wTrtlXl6RS3J0MEPiw3r8dpBd2YnpZREpEpEOd1cSkQki0iAit7iuPGN6h8iQAF6fP5m0mFDu+/N6lrZanXLpriLmPLWc5XuKeeLKIcRHBNE3Iojk6OCz+t0/2n70rBUxK2rq+d2SPVw2rC8L7hpPcIAvN41LZNqgGP7nw10cq6y9oPrf3lRAgJ8P35kzFFXY7KHz9lsvPXCm9LgwDpfVtNtNtjW/nOy843x16gCGJ0S4tEusO3TlzP0V4KqOGoiIL/Ar4F8uqMmYXik2LJBFD05mSHwYD7yazaxfL+XLC1Zz0zMrufeV9aDw8lezuGdqWvNjxqVEsfHQF331ZdV1fGPRJh5ZuLHFevavrc2jpq6RJ+YMbZ6CKSL8/IZR1DU28dN/7Djvuhsam3hv6xEuH96X6YNj8ZHzH+g9X01NelF2Q+o43ENpUsgrbXsBsT+tPEhYoB9fzkpiSHw4BeUnqerB33C6ss3ecqCzxY6/DrwFdH0xbWM8UFRoAAsfmMzXZqUzJikSEThZ38QPvzScjx6fyaXD4lsM5o1PjeJY5SkKnMG2eGM+dY1NFJSf5BXnwF1tfSN/WnmQmUPizhrITYsN5aGZA/lg29FOBwPbs2JfCSUn6rh+bCLhQf4M7Rdx0ZdTeGdLATP/Z2nzEg/doa6hiarahg7P3IE2lyEoqqzlva1HuGV8kuP/UXw4AHt78JIFF9znLiKJwI3As11oO19EskUku7jYtdO4jOkp+gT788Scofzh9kz++tAUPnxsBg/MGNjmJuKn+9035DnO3hetO8S4lEguG9aXP366j9ITp/j7pgJKTpzi4ZkDz3o8wF1TUvHzERau7Vrfe31jE29vKmD30SpUlbc3FdAn2J/ZQ+MAGJ/qWE6h0QUDteCYO1/QyVn52gNlNDQpH+885pLXbEt7V6eeNiD29Fz3swP7tTV5NDQpX3V+6xrazxHue3pw14wrBlSfAr6rqp2udK+qz6tqlqpmxcXFueCljendhvULJyTAl415x1l/8Dj7i6u5bWIK3587nJr6Rn63ZA8vLD/AqMQ+LS6cOlPf8CDmjOzHmxvy292a8EzvbT3C43/dzJynljPtl5/y4fajzB2VQKCf41qA8alRnDjV4LIBw5+8u5OrnlpOUQfjAlucV/d2Z7ifvvo0pp1wDw30o3+foLNmzNTWN/La2kNcNiyeNOcHQGJkMCEBvuzuwYOqrgj3LOB1ETkI3AI8IyI3uOB5jfF4fr4+jE2OZOOhchatO0R4oB/XjHbMvLljYgqvrTnEgZJqHpo1sM017E+7a1IqFSfrW2wukldazWtr8s6aKrlkZxFx4YH88qZRjErqQ2xYIHdO+mLK6PgUx4qcXel3V9UO5/Y3NSlLdxdRVdvAzz/IabPNybpG9hyrIjTAl+y8sm5bAuD080a1E+7gmDHTunvr3c1HKKuu477pac23+fgIg+PDe/SMmQsOd1UdoKppqpoGvAk8oqpvX3BlxniJcSlR7Cys5P1thdyQmUhIgOPqyccuH0xYoB/J0cFc1ckSDJMHRjOob1jzRiXHKmu544W1/PDt7S22K6xraGL5nmIuG9aX2yam8Ny8LFZ+79IWSyknRwcTGxbY6RTNk3WN3PDMKn76j53tttl+pIKy6joyEiJ4Z/MRVu07e5rozsIKGpuU+6cPoEk5a6aRq5R2cuYOMDQ+nJzCKnY6rz1QVV5emcuwfuFMGRjTqm3Yec11v1jLO3RlKuQiYDUwVETyReR+EXlYRB7u/vKM8XzjU6NobFLqGpq4/YyLrmLDAvnzfRNYcNf45hky7RER7pqUwpb8ClbtK+Gel9dRXlNHsL8vizd+sZNU9sEyqk41cNnw+A6fa3xqJBs6GVT9r/d3suWw4xtHe/PiP9tdjAi8cE8WKdEh/PCd7dQ1tOzB3XLY0SVzx6RU4iMCWZLjuq6ZNzfkN193cLy64z53gIdmpRMV6s/XFm6g4mQ9q/eXsutoFfdNH3DWN6ch8eGUnDjV5uJvdQ1NPLts/1kzgBqblC8/t7rT3cJcoSuzZW5X1QRV9VfVJFV9SVUXqOqCNtp+VVXf7J5SjfFMmSmOhcfGJEeetVTC+NRoRvTveIOS024an0Swvy9f/dN69hWdYMG88cwdlcAH2wqb++KX5BQR4OfDtEFt999/8bpR5JXWUNLOqpUfbT/KwrWHmD00jlMNTXzYanOU0z7bU8yoxD4kRgbz0+tHcKC4+qxNzLfklxMfEUi/PkFcPjyez/YUd2nsoDMNjU386J3tPPhqNruOVlJaXYeI45qE9sSFB/LHO8ZRcPwk335jCy+tyCUmNIDrxvQ/q23zoOqxs8/eF607xK8+2sW339zS4kz9r+sPs/7g8XaXnXYlu0LVGDeLDAngO1cN5d/nDr+g54kI8ueGzETqGpv4za1jmDE4jpvGJVJ1qoElOcdQVT7ZdYxp6THNXT/tGZ/qmMWzPvfsWdCFFSf53uKtjErsw/PzskiPC+WtDWfvM1tRU8/GQ8eZOdgxeeKSoX2ZMyKepz/d1+JMf2t+BWOSHB9wl2fEU1PXyOoD57bq4hvZh8/a4DqnsIqaukZONTTy8F82kFdaTWSw/1lLR7SWlRbN9+cO5187j/HJriLunJza5sJzp6dDtu53r6yt5/ef7CUyxJ+V+0p5b6vjg6+8po5f/3MXEwdEc83ohHN6f+fDwt2YHuCR2YOatxa8ED+6JoP3vj6dGzITAcfqlf0igli8sYD9xdXkldZwaQddMqeNTOxDv4gg/uOdHS2mBpZV1/Howo3UNTTx+9vGEuDnw03jklh3sIxDrS7+Wbm/hCaFWUO/mBn36CWDOFnf2LyOTcXJ+uZF1MCxu1VIgC9LzmHWzO6jVXz7za08s2xfi9uz8xwfTE/dlkn+8ZO8s/lIh10yZ7pvWhpfGp1ASIAvd01ue32iuPBAIkP82dVqOuRzn+2nrLqOV+6dyMjECP7r/Z2cONXA7z7eQ8XJen5y7YgOB8ddxcLdGA8SHODbYnDU10e4PrM/n+0p5o1sx9aDlw7r2+nzBPr58toDkwDljhfWcLCkms2Hy7nmD5+z/Uglv7l1TPPG0jdmJiLiWFXyTJ/tLiY8yK95vXuAUYl9GJ4QwevO9XC2OadAjk5y1Bzk78usIXEsyTnW5QXRFm90vO7yPSUtukCyDx4nMTKY68b054dfcnwr6mq4iwj/e1smn3/nEvqGt71RjIgwpNWMmcKKk7z4eS7Xj+3P2ORIfnb9SIqqTvHNv27mL2vyuGtyqstWKe2MhbsxHu6mzCQam5QXV+QyPCGCxMjgLj1uUN8wFj4wmfpG5ZYFq7l1wSp8fIS3Hp7K3FFfdCv0jwxmanoMizcWNIerqvLZnmKmD4ptMRgsItw+MZkdRyrZXlDBlnzHGjajE7/4ALh8eDzHKk+xtaDzna0am5S/byog0M+HgvKT5DqvcFVVsvPKmruX7pmaxsOz0rlubGKX3js4pjvGhAV22GZofDh7nBeDAfzu4z2owhNXOjaHyUyJ4rYJyXy88xgRwf782xVDuvz6F8rC3RgPN7RfOCP6R9DYpFzWhbP21o997f5JqCozBsfx3tenMyrp7AHem8clcaishmzn9Mk9x05wtLKWWUPOvljx+rGJBPn7sGjdIbbml5MWE0KfkC8GGC8fHo+fj/Dh9rYHac+0cl8JRVWnePxyR2h+7pwZk3/8JMcqTzEhzRHuIsL3rh7GvMmp5/T+OzOkXzhVpxp48NUNXPHkZ/wtO5+7p6SSHP3FJjPfmTOMMUl9+Ol1IzoczHW1jkdVjDEe4eZxSew4spPLMzrvb28to38Ea35wGf4dTMecM6IfIQHb+Y+3tzNzSFzzcgMz2wj3PsH+zB2VwDubjxDk78vUVlfe9gnxZ9qgWD7cdpTvXTWsuX+6sUl5f1shswbHNX8YLN6YT0SQH/dOS2PRukN8vreYe6amNfe3j0+98HGMjkwaEE1IgC97i6oY3DecuaMSmN9qmYio0ADe+X/Tu7WOtli4G+MF5k1x9PWOPaP/+1x0FOzguHT/m5cP4fX1h3hl1UHqGpoYmRhB/3a6gG6fmMLijQWcONXQPJh6prmj+vHdt7ax40hl8xjC+9sK+caiTYxJjmThA5MA+GjHUW4al0SQvy8zh8Ty940F1DU0kX3wOOGBfs3TFbvLkPhwdvx0zkUZID1XFu7GeAF/X58W+752hwdnDuTBmQNpalKOVdWetU/pmbJSo0iPc+x8NKaNbp4rMvrxg79v54NthYxM7IOqsmDZfmLDAtleUMH8V7OZOyqB2vombh6XBMCMwXG8tuYQGw8dJ/vgcTJTozqd9ugKPTHYwfrcjTEu5uMjJPQJJjyo/Qt1RIT7pw8kOjSgzYu0okMDmJoewwfbClFVVuwrYWdhJd+eM4Tf3DqaVftL+fG7O0iLCWGc8yKwKekx+PoI7209wp6iKrKcg6neys7cjTFuccekFL4yIbnds+urRybwg79vI6ewiuc+O0Df8EBuyEwk0M+XypMN/PjdHdwyPqn5zDkiyJ9xKZH8bX0+qnh9uNuZuzHGbTrqNrlyRDw+Ar/5125W7CvhvukDmpclvmdqGh89PoOvzR7U4jEzBsdR19iEr48wNuX8xhc8hYW7MaZHig0LZNKAGD7dVUR4oB93TGp5peiwfhFnfTjMGBwLwIj+EZ0useDpLNyNMT3W3FGOpY7vmJxCRAd9+KeNTookMTKY2W1MwfQ23v3RZozp0a7PTGR/cTUPzUzvUntfH2HJv81qc0tDb2PhbozpsSKC/PnJdSPO6THBAWev4OiNurJZx8siUiQi29u5/04R2Soi20RklYiMcX2ZxhhjzkVXvru8AlzVwf25wCxVHQX8DHjeBXUZY4y5AJ12y6jqchFJ6+D+VWf8ugZIuvCyjDHGXAhXjzrcD3zY3p0iMl9EskUku7i42MUvbYwx5jSXhbuIXIIj3L/bXhtVfV5Vs1Q1Ky7OpioZY0x3cclsGREZDbwIXK2q57b5oTHGGJe74DN3EUkBFgPzVHXPhZdkjDHmQnV65i4ii4DZQKyI5AM/BvwBVHUB8CMgBnjGuYBPg6pmdVfBxhhjOidnbih7UV9YpBjIO8+HxwIlLiynt/DG9+2N7xm8831743uGc3/fqara6aCl28L9QohItjd+O/DG9+2N7xm8831743uG7nvftgCDMcZ4IAt3Y4zxQL013L11iQNvfN/e+J7BO9+3N75n6Kb33Sv73I0xxnSst565G2OM6UCvC3cRuUpEdovIPhH5nrvr6Q4ikiwiS0Vkp4jsEJHHnLdHi8jHIrLX+V+P3AFYRHxFZJOIvOf8fYCIrHUe87+KSIC7a3QlEYkUkTdFZJeI5IjIFG841iLyTeff7+0iskhEgjzxWLe1bHp7x1cc/uB8/1tFZNz5vm6vCncR8QX+CFwNZAC3i0iGe6vqFg3At1Q1A5gMPOp8n98DPlHVwcAnzt890WNAzhm//wr4naoOAo7jWMPIk/we+EhVhwFjcLx3jz7WIpIIfAPIUtWRgC9wG555rF/h7GXT2zu+VwODnX/mA8+e74v2qnAHJgL7VPWAqtYBrwPXu7kml1PVQlXd6Py5Csc/9kQc7/XPzmZ/Bm5wT4XdR0SSgC/hWKsIcVz2fCnwprOJR71vEekDzAReAlDVOlUtxwuONY4r5INFxA8IAQrxwGOtqsuBslY3t3d8rwdeVYc1QKSIJJzP6/a2cE8EDp/xe77zNo/lXEs/E1gLxKtqofOuo0C8m8rqTk8B3wGanL/HAOWq2uD83dOO+QCgGPiTsyvqRREJxcOPtaoWAL8BDuEI9QpgA559rM/U3vF1Wcb1tnD3KiISBrwFPK6qlWfep45pTh411UlErgGKVHWDu2u5iPyAccCzqpoJVNOqC8ZDj3UUjrPUAUB/IJSOd3zzWN11fHtbuBcAyWf8nuS8zeOIiD+OYF+oqoudNx87/RXN+d8id9XXTaYB14nIQRxdbpfi6I+OdH51B8875vlAvqqudf7+Jo6w9/RjfTmQq6rFqlqPY2XZaXj2sT5Te8fXZRnX28J9PTDYOaIegGMA5l031+Ryzn7ml4AcVX3yjLveBe5x/nwP8M7Frq07qer3VTVJVdNwHNtPVfVOYClwi7OZR71vVT0KHBaRoc6bLgN24uHHGkd3zGQRCXH+fT/9vj32WLfS3vF9F7jbOWtmMlBxRvfNuVHVXvUHmAvsAfYD/+7uerrpPU7H8TVtK7DZ+Wcujv7nT4C9wBIg2t21duP/g9nAe86fBwLrgH3AG0Cgu+tz8XsdC2Q7j/fbQJQ3HGvgp8AuYDvwFyDQE481sAjHuEI9jm9q97d3fAHBMSNwP7ANx2yi83pdu0LVGGM8UG/rljHGGNMFFu7GGOOBLNyNMcYDWbgbY4wHsnA3xhgPZOFujDEeyMLdGGM8kIW7McZ4oP8PFC7O282FEUUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
