{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with spaCy\n",
    "\n",
    "SpaCy is a modern natural language processing library for Python https://spacy.io/\n",
    "\n",
    "We follow material found on offical github https://github.com/explosion/spacy-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, install spaCy library and language-specific resources, if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy\n",
    "#!python3 -m spacy download it # where \"it\" is Italian language code, use \"en\" for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import library and language resources in python (it might take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples are based on the notebook available at:\n",
    "https://github.com/explosion/spacy-notebooks/blob/master/notebooks/conference_notebooks/pycon_nlp/00_spacy_intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nothing but a quick overwiev of some features of spaCy, with the goal of getting familiar with the library.\n",
    "\n",
    "We will quickly cover tokenizer, sentence splitting, POS-tagging, syntactic dependencies, named entities and word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having loaded language-specific resources into nlp object, we can call it on text to automatically process it and transform it into a \"document\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Una frase di esempio. NLP in poche righe di codice.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = u\"Una frase di esempio. NLP in poche righe di codice.\"\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Una frase di esempio. NLP in poche righe di codice."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_doc = nlp(example_text)\n",
    "example_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doc object has a number of useful methods. Compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first position of example_text is U\n"
     ]
    }
   ],
   "source": [
    "print('first position of example_text is {}'.format(example_text[0])) # display result of indicing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first position of example_doc is Una\n"
     ]
    }
   ],
   "source": [
    "print('first position of example_doc is {}'.format(example_doc[0])) # display result of indicing doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's tokenized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another examples with multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = u\"Questa è la prima frase. Questa è la seconda, mentre questa è la terza.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc = nlp(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questa è la prima frase.\n",
      "Questa è la seconda, mentre questa è la terza.\n"
     ]
    }
   ],
   "source": [
    "for sent in example_doc.sents: # .sents method split sentences\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questa - PRON\n",
      "è - VERB\n",
      "la - DET\n",
      "prima - ADJ\n",
      "frase - NOUN\n",
      ". - PUNCT\n",
      "Questa - PRON\n",
      "è - VERB\n",
      "la - DET\n",
      "seconda - ADJ\n",
      ", - PUNCT\n",
      "mentre - SCONJ\n",
      "questa - PRON\n",
      "è - VERB\n",
      "la - DET\n",
      "terza - ADJ\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in example_doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that walks up the syntactic tree of the given token\n",
    "# and collects all tokens to the root token (including root token).\n",
    "\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: Spacy token\n",
    "    :return: list of Spacy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head != token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "\n",
    "    return tokens_to_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questa --> [Questa, frase]\n",
      "è --> [è, frase]\n",
      "la --> [la, frase]\n",
      "prima --> [prima, frase]\n",
      "frase --> []\n",
      ". --> [., frase]\n",
      "Questa --> [Questa, seconda]\n",
      "è --> [è, seconda]\n",
      "la --> [la, seconda]\n",
      "seconda --> []\n",
      ", --> [,, seconda]\n",
      "mentre --> [mentre, terza, terza, seconda]\n",
      "questa --> [questa, terza, terza, seconda]\n",
      "è --> [è, terza, terza, seconda]\n",
      "la --> [la, terza, terza, seconda]\n",
      "terza --> [terza, seconda]\n",
      ". --> [., seconda]\n"
     ]
    }
   ],
   "source": [
    "# For every token in document, print its tokens to the root\n",
    "for token in example_doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questa-nsubj-> frase-ROOT\n",
      "è-cop-> frase-ROOT\n",
      "la-det-> frase-ROOT\n",
      "prima-amod-> frase-ROOT\n",
      "\n",
      ".-punct-> frase-ROOT\n",
      "Questa-nsubj-> seconda-ROOT\n",
      "è-cop-> seconda-ROOT\n",
      "la-det-> seconda-ROOT\n",
      "\n",
      ",-punct-> seconda-ROOT\n",
      "mentre-mark-> terza-advcl-> terza-advcl-> seconda-ROOT\n",
      "questa-nsubj-> terza-advcl-> terza-advcl-> seconda-ROOT\n",
      "è-cop-> terza-advcl-> terza-advcl-> seconda-ROOT\n",
      "la-det-> terza-advcl-> terza-advcl-> seconda-ROOT\n",
      "terza-advcl-> seconda-ROOT\n",
      ".-punct-> seconda-ROOT\n"
     ]
    }
   ],
   "source": [
    "# Print dependency labels of the tokens\n",
    "for token in example_doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parc Güell - LOC\n",
      "Barcellona - LOC\n",
      "Fiat - ORG\n",
      "Salvador Dalì - PER\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with their correspondin named entity types\n",
    "\n",
    "example_doc = nlp(u\"Sono andato al Parc Güell di Barcellona guidando una Fiat, per incontrare Salvador Dalì.\")\n",
    "for ent in example_doc.ents: # ents method\n",
    "    print('{} - {}'.format(ent, ent.label_)) # label_ method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# use noun_chunks method\n",
    "example_doc = nlp(u\"Il mio amico Salvador ha dipinto un orologio sciolto sulla mia Fiat nuova.\")\n",
    "print([chunk for chunk in example_doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure why it doesn't work. Let's try it in English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My friend, Salvador, a melted clock, my new Fiat]\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"en\")\n",
    "example_doc2 = nlp2(u\"My friend Salvador painted a melted clock on my new Fiat.\")\n",
    "print([chunk for chunk in example_doc2.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So maybe it's not implemented for Italian yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download full spaCy model including English pre-trained word vectors:\n",
    "\n",
    "(As far as I understood, the Italian model has only context-dependent vectors forthe time being, i.e. based on POS, NER etc. Cf. https://spacy.io/models/it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77809423\n",
      "0.11093954\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "nlp3 = spacy.load('en_core_web_md')\n",
    "example_doc3 = nlp3(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = example_doc3[0]\n",
    "oranges = example_doc3[2]\n",
    "boots = example_doc3[6]\n",
    "hippos = example_doc3[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
